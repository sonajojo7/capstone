{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import os\n",
    "import configparser\n",
    "import datetime as dt\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import isnan, when, count, col, udf, dayofmonth, dayofweek, month, year, weekofyear\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import requests\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "\n",
    "import utility\n",
    "import functions\n",
    "\n",
    "import importlib\n",
    "importlib.reload(utility)\n",
    "from utility import clean_spark_immigration_data, clean_spark_temperature_data\n",
    "from utility import clean_spark_demographics_data, print_formatted_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['KEYS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['KEYS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "    config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "    config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "    enableHiveSupport().getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The objective of this project is to make analysis on the immigration events based on various datasets obtained. An ETL is to be created for I-94 immigration, global land temperature and US demographics datasets. This is used to get an idea of immigration patterns to US.\n",
    "#### Immigration Dataset\n",
    "This data comes from the US National Tourism and Trade Office.In the past all foreign visitors to the U.S. arriving via air or sea were required to complete paper Customs and Border Protection Form I-94 Arrival/Departure Record or Form I-94W Nonimmigrant Visa Waiver Arrival/Departure Record and this dataset comes from this forms. This dataset forms the core of the data warehouse and the customer repository has a years worth of data for the year 2016 and the dataset is divided by month. For this project the data is in a folder located at ../../data/18-83510-I94-Data-2016/. Each months data is stored in an SAS binary database storage format sas7bdat. For this project we have chosen going to work with data for the month of April. The data extraction, transformation and loading utility functions have been designed to work with any month's worth of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "immig_name = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "immigration_df =spark.read.format('com.github.saurfang.sas.spark').load(immig_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print_formatted_float(immigration_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_df.select(\"visapost\").dropDuplicates().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### World Temperature Data\n",
    "This dataset came from Kaggle accessible through '../../data2/GlobalLandTemperaturesByCity.csv' . The dataset provides data about global land temperatures by cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "file_name = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "temperature_df = spark.read.csv(file_name, header=True, inferSchema=True)\n",
    "# display the first five records\n",
    "temperature_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check the total number of records\n",
    "print_formatted_float(temperature_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### US Demographic Data\n",
    "This data comes from OpenSoft. It contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. The original data source is the US Census Bureau's 2015 American Community Survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "file_name = \"us-cities-demographics.csv\"\n",
    "demographics_df = spark.read.csv(file_name, inferSchema=True, header=True, sep=';')\n",
    "# display the first five records\n",
    "demographics_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check the total number of records\n",
    "print_formatted_float(demographics_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.builder.\\\n",
    "# config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "# config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "# enableHiveSupport().getOrCreate()\n",
    "\n",
    "# df_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# #write to parquet\n",
    "# df_spark.write.parquet(\"sas_data\")\n",
    "# df_spark=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data\n",
    "\n",
    "### 1.Immigration dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n",
    "\n",
    "files = os.listdir('../../data/18-83510-I94-Data-2016/')\n",
    "files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Data Dictionary - Immigaration Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "imm_dict =pd.read_csv('Data dictionaries/immigration-data-dictionary.csv')\n",
    "print(imm_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# columns with large amount of missing values as discovered by the exploratory analysis \n",
    "cols = ['occup', 'entdepu','insnum']\n",
    "\n",
    "# drop these columns\n",
    "new_immig_df = immigration_df.drop(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# display the new schema\n",
    "new_immig_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# drop duplicate entries\n",
    "new_immig_df = new_immig_df.dropDuplicates(['cicid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get a count after dropping duplicates\n",
    "print_formatted_float(new_immig_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# drop rows with missing values\n",
    "new_immig_df = new_immig_df.dropna(how='all', subset=['cicid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get a count after dropping rows with missing values\n",
    "print_formatted_float(new_immig_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# clean the immigration dataframe\n",
    "new_immigration_df = utility.clean_spark_immigration_data(immigration_df)\n",
    "\n",
    "print ('The shape of new immigration dataset :')\n",
    "new_immigration_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2.Temperature Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# print temperature dataframe schema\n",
    "temperature_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Data Dictionary - Temperature Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temp_dict =pd.read_csv('Data dictionaries/Temperature-data-dictionary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(temp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Exploratory analysis showed some missing values of average temperature.Hence clean global temperature data by dropping the rows that has missing average temperature values and dropping duplicate columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# clean the data by dropping rows with missing average temperature values and dropping duplicates\n",
    "new_temperature_df = utility.clean_spark_temperature_data(temperature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# count the number of records in dataset\n",
    "print_formatted_float(demographics_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 3.Demographics Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# print demographics database schema\n",
    "demographics_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Data Dictionary - Demographics Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demo_dict =pd.read_csv('Data dictionaries/Demographic data dictionary.csv')\n",
    "print(demo_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Exploratory analysis showed very minimal missing values in the demographics dataset.Hence clean demographics data by dropping the rows that contain missing values and dropping duplicate columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# clean demographics data by dropping the rows that contain missing values and dropping duplicate columns\n",
    "new_demographics_df = utility.clean_spark_demographics_data(demographics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "\n",
    "\n",
    "### Database schema\n",
    "#### Fact Table\n",
    "- The immigration fact table is the fact table of the data model. This table's data comes from the immigration data sets and contains keys that links to the dimension tables. \n",
    "\n",
    "#### Dimension Tables\n",
    "- The *country* dimension table is made up of data from the global land temperatures by city and the immigration datasets. The combination of these two  datasets allows analysts to study correlations between global land temperatures and immigration patterns to the US.\n",
    "\n",
    "- The *us_demographics* dimension table comes from the demographics dataset and links to the immigration fact table at US state level. This dimension would allow analysts to get insights into migration patterns into the US based on demographics as well as overall population of states.It gives an insight on which states most immigrants are drawn towards\n",
    "\n",
    "- The *immigration_calendar* dimension table formed from the immigration datasets points towards arrival time of the immigrants\n",
    "\n",
    "- The *visa_type* dimension table comes from the immigration datasets and links to the immigaration making use of the visa_type_key.\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    " **The pipeline steps are as follows:**\n",
    "\n",
    "- Run functions.py and utility.py in the console to enable the functions for cleaning the datasets and creating the tables\n",
    "- Load the datasets\n",
    "- Cleaning the I94 Immigration data to create Spark dataframe for each month\n",
    "- Create visa_type dimension table\n",
    "- Create calendar dimension table\n",
    "- Extract clean global temperatures data\n",
    "- Create country dimension table\n",
    "- Create immigration fact table\n",
    "- Load demographics data\n",
    "- Clean demographics data\n",
    "- Create demographic dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import image module\n",
    "from IPython.display import Image\n",
    "  \n",
    "# get the image\n",
    "Image(url=\"Data model.png\", width=700, height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### I. Create the immigration calendar dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here\n",
    "\n",
    "def create_immigration_calendar_dimension(df, output_data):\n",
    "    \"\"\"This function creates an immigration calendar based on arrival date\n",
    "    \n",
    "    :param df: spark dataframe of immigration events\n",
    "    :param output_data: path to write dimension dataframe to\n",
    "    :return: spark dataframe representing calendar dimension\n",
    "    \"\"\"\n",
    "    # create a udf to convert arrival date in SAS format to datetime object\n",
    "    get_datetime = udf(lambda x: (dt.datetime(1960, 1, 1).date() + dt.timedelta(x)).isoformat() if x else None)\n",
    "    \n",
    "    # create initial calendar df from arrdate column\n",
    "    calendar_df = df.select(['arrdate']).withColumn(\"arrdate\", get_datetime(df.arrdate)).distinct()\n",
    "    \n",
    "    # expand df by adding other calendar columns\n",
    "    calendar_df = calendar_df.withColumn('arrival_day', dayofmonth('arrdate'))\n",
    "    calendar_df = calendar_df.withColumn('arrival_week', weekofyear('arrdate'))\n",
    "    calendar_df = calendar_df.withColumn('arrival_month', month('arrdate'))\n",
    "    calendar_df = calendar_df.withColumn('arrival_year', year('arrdate'))\n",
    "    calendar_df = calendar_df.withColumn('arrival_weekday', dayofweek('arrdate'))\n",
    "\n",
    "    # create an id field in calendar df\n",
    "    calendar_df = calendar_df.withColumn('id', monotonically_increasing_id())\n",
    "    \n",
    "    # write the calendar dimension to parquet file\n",
    "    partition_columns = ['arrival_year', 'arrival_month', 'arrival_week']\n",
    "    calendar_df.write.parquet(output_data + \"immigration_calendar\", partitionBy=partition_columns, mode=\"overwrite\")\n",
    "    \n",
    "    return calendar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "output_data = \"tables/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "calendar_df = create_immigration_calendar_dimension(new_immigration_df, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "calendar_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### II. Create the country dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_country_dimension_table(df, temp_df, output_data):\n",
    "    \"\"\"This function creates a country dimension from the immigration and global land temperatures data.\n",
    "    \n",
    "    :param df: spark dataframe of immigration events\n",
    "    :temp_df: spark dataframe of global land temperatures data.\n",
    "    :param output_data: path to write dimension dataframe to\n",
    "    :return: spark dataframe representing calendar dimension\n",
    "    \"\"\"\n",
    "    # get the aggregated temperature data\n",
    "    agg_temp = utility.aggregate_temperature_data(temp_df).toPandas()\n",
    "    # load the i94res to country mapping data\n",
    "    mapping_codes = pd.read_csv('i94.csv')\n",
    "    \n",
    "    @udf('string')\n",
    "    def get_country_average_temperature(name):\n",
    "        print(\"Processing: \", name)\n",
    "        avg_temp = agg_temp[agg_temp['Country']==name]['average_temperature']\n",
    "        \n",
    "        if not avg_temp.empty:\n",
    "            return str(avg_temp.iloc[0])\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    @udf()\n",
    "    def get_country_name(code):\n",
    "        name = mapping_codes[mapping_codes['code']==code]['name'].iloc[0]\n",
    "        \n",
    "        if name:\n",
    "            return name.title()\n",
    "        return None\n",
    "        \n",
    "    # select and rename i94res column\n",
    "    dim_df = df.select(['i94res']).distinct() \\\n",
    "                .withColumnRenamed('i94res', 'country_code')\n",
    "    \n",
    "    # create country_name column\n",
    "    dim_df = dim_df.withColumn('country_name', get_country_name(dim_df.country_code))\n",
    "    \n",
    "    # create average_temperature column\n",
    "    dim_df = dim_df.withColumn('average_temperature', get_country_average_temperature(dim_df.country_name))\n",
    "    \n",
    "    # write the dimension to a parquet file\n",
    "    dim_df.write.parquet(output_data + \"country\", mode=\"overwrite\")\n",
    "    \n",
    "    return dim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "country_dim_df = create_country_dimension_table(new_immigration_df, new_temperature_df, output_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "country_dim_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### III. Create the visa_type dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_visa_type_dimension_table(df, output_data):\n",
    "    \"\"\"This function creates a visa type dimension from the immigration data.\n",
    "    \n",
    "    :param df: spark dataframe of immigration events\n",
    "    :param output_data: path to write dimension dataframe to\n",
    "    :return: spark dataframe representing calendar dimension\n",
    "    \"\"\"\n",
    "    # create visatype df from visatype column\n",
    "    visatype_df = df.select(['visatype']).distinct()\n",
    "    \n",
    "    # add an id column\n",
    "    visatype_df = visatype_df.withColumn('visa_type_key', monotonically_increasing_id())\n",
    "    \n",
    "    # write dimension to parquet file\n",
    "    visatype_df.write.parquet(output_data + \"visatype\", mode=\"overwrite\")\n",
    "    \n",
    "    return visatype_df\n",
    "\n",
    "def get_visa_type_dimension(output_data):\n",
    "    return spark.read.parquet(output_data + \"visatype\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "visatype_df = create_visa_type_dimension_table(new_immigration_df, output_data)\n",
    "visatype_df.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### IV. Create the demographics dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_demographics_dimension_table(df, output_data):\n",
    "    \"\"\"This function creates a us demographics dimension table from the us cities demographics data.\n",
    "    \n",
    "    :param df: spark dataframe of us demographics survey data\n",
    "    :param output_data: path to write dimension dataframe to\n",
    "    :return: spark dataframe representing demographics dimension\n",
    "    \"\"\"\n",
    "    dim_df = df.withColumnRenamed('Median Age','median_age') \\\n",
    "            .withColumnRenamed('Male Population', 'male_population') \\\n",
    "            .withColumnRenamed('Female Population', 'female_population') \\\n",
    "            .withColumnRenamed('Total Population', 'total_population') \\\n",
    "            .withColumnRenamed('Number of Veterans', 'number_of_veterans') \\\n",
    "            .withColumnRenamed('Foreign-born', 'foreign_born') \\\n",
    "            .withColumnRenamed('Average Household Size', 'average_household_size') \\\n",
    "            .withColumnRenamed('State Code', 'state_code')\n",
    "    # lets add an id column\n",
    "    dim_df = dim_df.withColumn('id', monotonically_increasing_id())\n",
    "    \n",
    "    # write dimension to parquet file\n",
    "    dim_df.write.parquet(output_data + \"demographics\", mode=\"overwrite\")\n",
    "    \n",
    "    return dim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demographics_dim_df = create_demographics_dimension_table(demographics_df, output_data)\n",
    "demographics_dim_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### V. Create the immigration Fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_immigration_fact_table(df, output_data):\n",
    "    \"\"\"This function creates an country dimension from the immigration and global land temperatures data.\n",
    "    \n",
    "    :param df: spark dataframe of immigration events\n",
    "    :param visa_type_df: spark dataframe of global land temperatures data.\n",
    "    :param output_data: path to write dimension dataframe to\n",
    "    :return: spark dataframe representing calendar dimension\n",
    "    \"\"\"\n",
    "    # get visa_type dimension\n",
    "    dim_df = get_visa_type_dimension(output_data).toPandas()\n",
    "    \n",
    "    @udf('string')\n",
    "    def get_visa_key(visa_type):\n",
    "        \"\"\"user defined function to get visa key\n",
    "        \n",
    "        :param visa_type: US non-immigrant visa type\n",
    "        :return: corresponding visa key\n",
    "        \"\"\"\n",
    "        key_series = dim_df[dim_df['visatype']==visa_type]['visa_type_key']\n",
    "        \n",
    "        if not key_series.empty:\n",
    "            return str(key_series.iloc[0])\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # create a udf to convert arrival date in SAS format to datetime object\n",
    "    get_datetime = udf(lambda x: (dt.datetime(1960, 1, 1).date() + dt.timedelta(x)).isoformat() if x else None)\n",
    "    \n",
    "    # rename columns to align with data model\n",
    "    df = df.withColumnRenamed('cicid','record_id') \\\n",
    "            .withColumnRenamed('i94res', 'country_residence_code') \\\n",
    "            .withColumnRenamed('i94addr', 'state_code') \n",
    "    \n",
    "    # create visa_type key\n",
    "    df = df.withColumn('visa_type_key', get_visa_key('visatype'))\n",
    "    \n",
    "    # convert arrival date into datetime object\n",
    "    df = df.withColumn(\"arrdate\", get_datetime(df.arrdate))\n",
    "    \n",
    "    # write dimension to parquet file\n",
    "    df.write.parquet(output_data + \"immigration_fact\", mode=\"overwrite\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_fact_df = create_immigration_fact_table(new_immigration_df, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_fact_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.2 Data Quality Checks\n",
    "The data quality checks are done to ensure that the ETL has created the fact table  and the dimension tables with adequate amount of data.\n",
    " \n",
    "#### Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "\n",
    "table_dfs = {\n",
    "    'immigration_fact': immigration_fact_df,\n",
    "    'visa_type_dim': visatype_df,\n",
    "    'calendar_dim': calendar_df,\n",
    "    'usa_demographics_dim': demographics_dim_df,\n",
    "    'country_dim': country_dim_df\n",
    "}\n",
    "for table_name, table_df in table_dfs.items():\n",
    "    # quality check for table\n",
    "    functions.quality_checks(table_df, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 5: Complete Project Write Up\n",
    "\n",
    "#### Rationale for the choice of tools and technologies for the project\n",
    " \n",
    " Apache Spark was the main tool used because it is able to handle files of different format and of large amounts of data.A faster unified analytics engine to handle big data\n",
    "\n",
    "#### Propose how often the data should be updated and why.\n",
    " \n",
    " The available I94 immigration dataset is being updated on a monthly basis . Thus our data also require monthly update.\n",
    "\n",
    "#### Write a description of how you would approach the problem differently under the following scenarios:\n",
    " 1. The data was increased by 100x.\n",
    "     - Spark has the advantage of being highly scalable. So if the data is increased, we can handle it by increasing the number of our cluser nodes       \n",
    " 2. The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "     -  In this kind of a situation,we can use  Apache Airflow to schedule and run data pipelines.\n",
    " 3. The database needed to be accessed by 100+ people.\n",
    "     -   Amazon Redshift can be made use of in such a scenario.The analytics database can be shifted to Amazon Redshift\n",
    " ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
